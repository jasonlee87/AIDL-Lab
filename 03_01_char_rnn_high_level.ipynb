{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "03_01_char_rnn_high_level.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonlee87/AIDL-Lab/blob/master/03_01_char_rnn_high_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t09eeeR5prIJ"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "GCCk8_dHpuNf",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# RNN 기반 Text Genration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BwpJ5IffzRG6"
      },
      "source": [
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>\n",
        "\n",
        "위 출력은 모델이 생성한 문장이다. \"Q\"로 시작하는 곳부터 Text Generation 한 경우이다.\n",
        "생성된 문장을 보면 일부 문장은 문법적 맞지만 대부분은 의미 파악이 어렵다. 하지만 훈련을 처음 시작했을때 모델은 알파벳도 그리고 이것이 텍스트의 기본 단위인지도 몰랐다는 점에 비춰보면 유의미한 결과라 할수 있다.\n",
        "\n",
        "이 예제를 통해 구현 과정을 살펴보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### TensorFlow와 관련 모듈들을 import 한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yG_n40gFzf9s",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### 세익스피어 데이타셋을 다운로드한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pD_55cOxLkAb",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Data 읽기\n",
        "\n",
        "하나의 텍스트로 묶는다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aavnuByVymwK",
        "outputId": "63b0d354-6db9-4f0c-e749-4f086348c3d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Duhg9NrUymwO",
        "outputId": "6c6b8054-8c30-4f63-f299-ea228d7b2724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])\n",
        "#print(text[len(text)-100:len(text)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlCgQBRVymwR",
        "outputId": "ebd18974-a397-4d0b-8870-973fcb85489b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pxnB5q7_KzF",
        "colab_type": "code",
        "outputId": "ae6238fc-e822-4bb5-cc70-8be0f1dc8381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "print (type(vocab))\n",
        "print (len(vocab))\n",
        "print (vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## 텍스트 가공"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### 텍스트를 벡터화한다\n",
        "\n",
        "학습하기 전에 문자를 숫자로 표기할 필요가 있다. 문자를 인덱스로, 인덱스를 문자로 맵핑한 2개의 lookup 테이블을 만든다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IalZLbvOzf-F",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "#print (type(char2idx)) # class 'dick'\n",
        "#print (type(idx2char)) # class 'numpy.ndarray'\n",
        "#print (type(text))     # class 'str'\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "#print (text_as_int)     # [18 47 56 ... 45 8 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "이제 각 문자마다 고유의 인덱스를 두었고, 이 예에서의 인덱스 범위는 0~64가 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FYyNlCNXymwY",
        "outputId": "d7149252-34a0-4c89-e137-b2a89df93bdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT6h6VnJF4TX",
        "colab_type": "text"
      },
      "source": [
        "예를 들어 첫 13문자가 어떻게 숫자로 표현되었는지 보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l1VKcQHcymwb",
        "outputId": "c7709408-ad45-4c28-9d3f-fa5af4b3903e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### 예측하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "일련의 캐릭터(시퀀스)가 주어졌을때 다음 캐릭터로 나올 가장 높은 확률의 문자는 뭘까? 우리가 할 과제가 바로 이를 수행할 모델을 학습 시키는것이다. 입력으로 문자열이 들어가면 모델은 그에 뒤따르는 문자열을 출력(예측)하는 것이다.\n",
        "\n",
        "RNN은이 앞서 시퀀스로부터 계산된 내부 상태(정보)를 예측에 활용한다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### 학습 예(training example) 만들기\n",
        "\n",
        "그런 다음 텍스트를 예제 시퀀스로 나눕니다. 각 입력 시퀀스마다 seq_length 문자를  포함한다. 출력 시퀀스 길이는 한 문자 오른쪽으로 이동한것을 제외하곤 입력 시퀀스 길이와 같다\n",
        "우린 Text를 'seq_length+1' 크기로 자를 것이다. 예를 들어 텍스트가 \"Hello\"라면 seq_length = 4 가 된다. 따라서 입력 시퀀스는 \"Hell\"(길이 4)이고 기대하는 출력 시퀀스도 길이 4인 \"ello\"가 되도록 했다.\n",
        "\n",
        "이 작업을 위해 tf.data.Dataset.from_tensor_slices 함수를 사용하여 텍스트 벡터를 일련의 문자 인덱스로 변환했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0UHJDA39zf-O",
        "outputId": "f78a23f5-5cf3-44bd-934a-712368ad5b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "print (char_dataset)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (), types: tf.int64>\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "각각의 문자를 원하는 크기의 시퀀스로 변환할때 batch method를 쓰면 편하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4hkDU3i7ozi",
        "outputId": "17478963-ec18-4e92-9092-cba2bc0c4de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#print (type(sequences))\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()]))) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "입출력 시퀀스를 map method로 묶어 dataset을 만든다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NGu-FkO_kYU",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hiCopyGZymwi"
      },
      "source": [
        "입력 시퀀스와 타겟 시퀀스 실제 모습(한 문자 shift 되었고 길이는 같음)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GNbw-iR0ymwj",
        "outputId": "d03497a4-f798-48d8-c3e2-f1c2e5b97125",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_33OHL3b84i0"
      },
      "source": [
        "이들 벡터의 각 인덱스가 매 타임스텝마다 처리된다. 타임스텝 0에서 'F'에 해당하는 인덱스를 받고, 다음 문자로 \"i\"가 출력되게끔 예측을 시도한다. 다음 타임스텝도 같은 일을 하겠지만 'RNN'은 이제부턴 다음의 입력 문자뿐만 아니라 문맥(지나온 동안의 시퀀스 내부 정보)을 함께 고려한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0eBu9WZG84i0",
        "outputId": "0f00bc49-a2d0-4dec-ea92-b562ab158bf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### 학습 배치 만들기\n",
        "\n",
        "우리는 텍스트를 운용가능한 시퀀스로 자르기 위해 'tf.data'를 사용할것이다. 이 데이타를 모델에 넣기(feeding)전에 먼저 데이타를 섞고, 배치단위로 취할것이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p2pGotuNzf-S",
        "outputId": "58e6af02-fc94-4f6f-9009-aa41084ead20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## 모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "'tf.keras.Squential'을 이용해서 모델을 정의했으며 이 예제에선 간략히 3개층으로 모델을 정의했다.\n",
        "* `tf.keras.layers.Embedding`: 입력층. 문자와 `embedding_dim` dimensions을 가진 문자 벡터가 연결하는 테이블을 가지고 있다\n",
        "* `tf.keras.layers.GRU`: `units=rnn_units` 크기를 가진 GRU 층\n",
        "* `tf.keras.layers.Dense`: `vocab_size` 출력을 가진 출력층."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zHT8cLh7EAsg",
        "outputId": "1406a4a9-bbd1-4225-96ba-80dd88126094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "print (vocab_size)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MtCrdfzEI2N0",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wwsrpOik5zhv",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RkA5upJIJ7W7"
      },
      "source": [
        "아래 그림은 문자가 입력되면 lookup table에 의해 입력 문자에 대응하는 embedding vector가 GRU의 입력으로 들어간다. 그리고 dense layer를 거처 다음 문자를 예측할수 있는 logit을 만드는 과정을 설명하고 있다.\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_training.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## 모델 예비 시도\n",
        "\n",
        "모델을 한번 실행해 보자 우리 예측대로 동작하는지. 그리고 먼저 출력의 shape를 체크해 보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-_70kKAPrPU",
        "outputId": "b9e73b65-65b5-4dfc-cfe9-52b40af5a0ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "(위 예에서 시퀀스 길이를 100으로 설정했지만 이 모델은 어떠한 시퀀스 길이도 가능하다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vPGmAAXmVLGC",
        "outputId": "30889c4b-08df-49f0-eca8-ed83997180e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "모델에서 실제 예측을 하가 위해선 출력 분포에서 인덱스를 샘플링해야 합니다. 이 분포는 문자 어휘에 대한 로짓(logit)으로 정의됩니다.\n",
        "\n",
        "배치 방식의 첫 번째 예를 봅시다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4V4MfFg0RQJg",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "매 timestep 마다 이렇게 입력을 받아 다음 문자를 예측하는것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YqFMUQc_UFgM",
        "outputId": "363b4415-6574-4c4e-9eb8-38e0685d7097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([59, 21,  3,  0, 22, 20, 63, 47, 48, 11, 51,  9, 26, 62, 37, 48, 52,\n",
              "       31, 13,  2, 46, 11, 55,  9, 26, 41, 26, 63, 28,  2, 60, 22,  6,  1,\n",
              "       20, 46, 54, 57, 41, 38, 21, 28, 38, 52,  8, 60, 14, 57, 33, 30, 28,\n",
              "       21, 25, 29, 41, 58, 60, 22, 11, 22, 44,  0, 55, 59, 14,  8, 38, 29,\n",
              "       38, 51, 54, 37, 54, 39,  9, 60, 20, 57, 51, 34, 19, 22, 37,  4, 28,\n",
              "       53, 43, 23,  7, 38, 43,  0,  1, 50,  9, 49, 63, 43,  6,  9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "학습되지 않은 모델로 결과를 한번 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWcFwPwLSo05",
        "outputId": "2d2b9663-68ae-4e46-d692-a30afc6b7f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'gainst our meaning, have prevented:\\nBecause, my lord, we would have had you heard\\nThe traitor speak,'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'uI$\\nJHyij;m3NxYjnSA!h;q3NcNyP!vJ, HhpscZIPZn.vBsURPIMQctvJ;Jf\\nquB.ZQZmpYpa3vHsmVGJY&PoeK-Ze\\n l3kye,3'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "이제부턴 이 문제를 표준 분류 문제로 간주할수있다. 즉 앞서 주어진 RNN 상태와 매 timestep 마다의 입력으로 새로운 문자의 클래스를 예측하는 분류 문제로 볼수 있는것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### 옵티마이저와 손실함수 선택\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "여기선 표준 `tf.keras.losses.sparse_categorical_crossentropy` 손실함수를 이용할 것이다. \n",
        "그리고 우리 모델이 logits를 리턴하기 때문에 'from_logits' flag 를 활성화시켰다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4HrXTACTdzY-",
        "outputId": "fb336446-622f-4c2e-cef0-8c05d7d0ca17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.175708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jeOXriLcymww"
      },
      "source": [
        "그리고 `tf.keras.Model.compile` method로 학습 설정들을 정의하는데 여기선 옵티마이저로 `tf.keras.optimizers.Adam`을 사용할 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDl1_Een6rL0",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### 체크포인트(파라미터) 저장 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C6XBUUavgF56"
      },
      "source": [
        "학습중 체크포인트를 저장하기 위해 `tf.keras.callbacks.ModelCheckpoint` 메소드를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6fWTriUZP-n",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### 학습 실행"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "여기선 10 epochs 학습시켰다. 대략 2:30분 소요되었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7yGBE2zxMMHs",
        "colab": {}
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UK-hmKjYVoll",
        "outputId": "360735d7-7e8a-42bf-d025-7bb6eda1120e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 972s 6s/step - loss: 2.6594\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 1021s 6s/step - loss: 1.9528\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 961s 6s/step - loss: 1.6842\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 940s 5s/step - loss: 1.5380\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 937s 5s/step - loss: 1.4514\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 979s 6s/step - loss: 1.3933\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 1006s 6s/step - loss: 1.3478\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 1013s 6s/step - loss: 1.3101\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 1031s 6s/step - loss: 1.2757\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 990s 6s/step - loss: 1.2436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## 텍스트 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JIPcXllKjkdr"
      },
      "source": [
        "### 최신 체크포인트 가져오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LyeYRiuVjodY"
      },
      "source": [
        "예측 과정을 들여다보기 위해 배치 크기를 1로 한다.\n",
        "이를 위해 'batch_size'로 재설계하고 체크포인트를 불러온다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zk2WJ2-XjkGz",
        "outputId": "5b90bf84-7b8a-4c62-87b5-fd32e24f9d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LycQ-ot_jjyu",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71xa6jnYVrAN",
        "outputId": "fd4154a6-2a78-41d8-e07f-7e8f17dbf54a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "### 예측 과정\n",
        "\n",
        "다음 코드 블럭이 텍스트를 생성한다.\n",
        "\n",
        "* 사전 약속한 시작 문자열을 만나면 예측을 시작한다. RNN 상태를 초기화하고 생성할 문자의 갯수를 정한다.\n",
        "\n",
        "* 시작 문자열과 RNN 상태를 이용 다음 문자의 예측 분포를 구한다.\n",
        "\n",
        "* 그리고 예측 문자의 인덱스를 구하기 위해 categorical distribution을 이용한다. 예측한 문자는 다음 단계의 입력으로 쓰인다.\n",
        "\n",
        "* RNN 상태는 다시 모델로 피드백되면서 더 많은 문맥을 가지게 된다. 다음 문자를 예측한 후엔 RNN 상태가 갱신되고 이는 다시 모델로 피드백된다. 앞서 예측 단어를 통해 문맥 정보를 얻어내는것 이것이 학습의 방법이다.\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/text_generation_sampling.png?raw=1)\n",
        "\n",
        "생성된 글을 보면 모델이 학습량 부족에도 불구하고 언제 대문자를 쓰는지, 언제 단락을 만드는지, 세익스피어처럼  글쓰기를 흉내 내고 있음을 볼 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktovv0RFhrkn",
        "outputId": "023f51a5-cf8f-42a3-8914-207d887fc635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: my lord.\n",
            "An hateou-latence that kiss my lord's death,\n",
            "knop we may could think no less chamberlage?\n",
            "O blawlich, go\n",
            "ather, wherefore has bore me:\n",
            "\n",
            "Second Senatarion:\n",
            "Made her than my soul profess'd to their appear?\n",
            "And I send the back of known in\n",
            "averenh peins. O, if I wounger his heart.\n",
            "Look, he hath a very crack'd in men,\n",
            "In brief,--\n",
            "Injeing, but\n",
            "Off the good god-hast be a thousand men than she will privare\n",
            "A great dear Jap, may far in thy contract,\n",
            "Unless I am stugh 'mongst our studment.\n",
            "\n",
            "Second Senator:\n",
            "So many counser take my lord;\n",
            "I have from the city.\n",
            "\n",
            "GREGO:\n",
            "Good, let my lady else?--but did not take off thou now,--\n",
            "Then if you well drunk both your yields to-night:\n",
            "A brainn mystery of this death?\n",
            "\n",
            "Second MusiBis men and unge-up the war;\n",
            "How was this cause that got delight, then bloody;\n",
            "One of this face of my husband's creatur-vicans that thou art a craf\n",
            "Thus faults to hours must not be gone;\n",
            "Poor brother--leapt in eyes, great Autiul father!\n",
            "The lobber that no more than contrare th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AM2Uma_-yVIq"
      },
      "source": [
        "성능을 올릴수 있는 가장 쉬운 방법은 학습량을 더 늘리는것이며(epochs=30) 또한 다른 RNN 층을 쓴다거나 예측 관련한 temperature 파라미터를 조정하는 방법도 가능할 것이다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y4QwTjAM6A2O"
      },
      "source": [
        "## 고급: 맞춤형 학습\n",
        "\n",
        "이 예는 간단한 반면 여러 제어 기능을 제공하지는 않습니다.\n",
        "\n",
        "앞의 예를 통해 모델을 수동으로 실행하는 방법을 살펴 봤으므로 이번에 학습 루프를 풀고 직접 구현하는 방법을 알아보고자 합니다. 예를 들어, 모델의 개방 루프 출력을 안정화하기 위해 커리큘럼 학습을 구현하는 경우 이 예제가 시작점이 될것입니다.\n",
        "\n",
        "tf.GradientTape를 사용하여 그라디언트를 추적합니다. Eager execution guide를 읽으면이 접근 방식에 대해 자세히 알아볼 수 있습니다.\n",
        "\n",
        "절차는 다음과 같이 작동합니다.\n",
        "\n",
        "먼저 RNN 상태를 초기화하십시오. tf.keras.Model.reset_states 메소드를 호출하여 이를 수행합니다.\n",
        "\n",
        "다음으로 데이터 집합을 반복하고 각 집합과 관련된 예측을 계산합니다.\n",
        "\n",
        "tf.GradientTape를 열고 해당 컨텍스트에서 예측 및 손실을 계산하십시오.\n",
        "\n",
        "tf.GradientTape.grads 방법을 사용하여 모델 변수와 관련하여 손실의 기울기를 계산하십시오.\n",
        "\n",
        "마지막으로 옵티마이저의 tf.train.Optimizer.apply_gradients 메소드를 사용하여 한 단계 아래로 이동하십시오.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_XAm7eCoKULT",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qUKhnZtMVpoJ",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4kH1o0leVIp",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d4tSNwymzf-q",
        "outputId": "456a4c20-8b9c-465f-accc-874314a94e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
            "Epoch 1 Batch 0 Loss 4.174903392791748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-JFlTebjf5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}